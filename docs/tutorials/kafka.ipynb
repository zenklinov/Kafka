{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenklinov/Kafka/blob/main/docs/tutorials/kafka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Robust machine learning on streaming data using Kafka and Tensorflow-IO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/kafka\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial focuses on streaming data from a [Kafka](https://kafka.apache.org/quickstart) cluster into a `tf.data.Dataset` which is then used in conjunction with `tf.keras` for training and inference.\n",
        "\n",
        "Kafka is primarily a distributed event-streaming platform which provides scalable and fault-tolerant streaming data across data pipelines. It is an essential technical component of a plethora of major enterprises where mission-critical data delivery is a primary requirement.\n",
        "\n",
        "**NOTE:** A basic understanding of the [kafka components](https://kafka.apache.org/documentation/#intro_concepts_and_terms) will help you in following the tutorial with ease.\n",
        "\n",
        "**NOTE:** A Java runtime environment is required to run this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required tensorflow-io and kafka packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "48B9eAMMhAgw",
        "outputId": "a26a216c-5ea9-454d-ef98-f4e228439523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.11/dist-packages (0.37.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-io) (0.37.1)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.11/dist-packages (2.2.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgO11GTJaTj"
      },
      "source": [
        "### Validate tf and tfio imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dX74RKfZ_TdF",
        "outputId": "f0eafe18-9bd8-44cf-bb10-33ab1b5dfd8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow-io version: 0.37.1\n",
            "tensorflow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "YUj0878jPyz7",
        "outputId": "3317ae93-b951-4372-c6fb-136c41bef13b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 84.0M  100 84.0M    0     0   254k      0  0:05:38  0:05:38 --:--:--  412k\n",
            "-rw-r--r-- 1 root root 85M May  5 08:17 kafka_2.13-3.1.0.tgz\n",
            "kafka_2.13-3.1.0.tgz: gzip compressed data, from FAT filesystem (MS-DOS, OS/2, NT), original size modulo 2^32 92016640\n"
          ]
        }
      ],
      "source": [
        "# hapus file lama kalau masih ada\n",
        "!rm -f kafka_2.13-3.1.0.tgz\n",
        "\n",
        "# unduh dari Apache Archive\n",
        "!curl -O https://archive.apache.org/dist/kafka/3.1.0/kafka_2.13-3.1.0.tgz\n",
        "\n",
        "# periksa ukuran & tipe file (harus ~84 M dan \"gzip compressed data\")\n",
        "!ls -lh kafka_2.13-3.1.0.tgz\n",
        "!file kafka_2.13-3.1.0.tgz\n",
        "\n",
        "# ekstrak\n",
        "!tar -xzf kafka_2.13-3.1.0.tgz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Using the default configurations (provided by Apache Kafka) for spinning up the instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "n9ujlunrWgRx",
        "outputId": "7f894a24-0ba7-46b6-aa18-bf94d1f20437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): kafka_2.13-3.1.0.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "Waiting 10 s for Kafka & ZooKeeper to be up…\n",
            "root       21482       1 17 08:20 ?        00:00:01 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.13-3.1.0/bin/../logs/zookeeper-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-3.1.0/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-3.1.0/bin/../config/log4j.properties -cp /content/kafka_2.13-3.1.0/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/audience-annotations-0.5.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-basic-auth-extension-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-file-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-json-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-mirror-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-mirror-client-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-runtime-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-transforms-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-annotations-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-core-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-databind-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-dataformat-csv-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-datatype-jdk8-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-jaxrs-base-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-jaxrs-json-provider-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-module-jaxb-annotations-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-module-scala_2.13-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/content/kafka_2.13-3.1.0/bin/../libs/javassist-3.27.0-GA.jar:/content/kafka_2.13-3.1.0/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jaxb-api-2.3.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-client-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-common-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-container-servlet-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-container-servlet-core-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-hk2-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-server-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-client-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-continuation-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-http-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-io-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-security-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-server-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-servlet-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-servlets-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-util-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-util-ajax-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jline-3.12.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/jose4j-0.7.8.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka_2.13-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-clients-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-log4j-appender-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-metadata-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-raft-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-server-common-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-shell-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-storage-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-storage-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-examples-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-scala_2.13-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-test-utils-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-tools-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/log4j-1.2.17.jar:/content/kafka_2.13-3.1.0/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/maven-artifact-3.8.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-buffer-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-codec-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-common-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-handler-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-resolver-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-native-epoll-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-native-unix-common-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-3.1.0/bin/../libs/plexus-utils-3.2.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-3.1.0/bin/../libs/rocksdbjni-6.22.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-collection-compat_2.13-2.4.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-java8-compat_2.13-1.0.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-library-2.13.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-logging_2.13-3.9.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-reflect-2.13.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/slf4j-api-1.7.30.jar:/content/kafka_2.13-3.1.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/content/kafka_2.13-3.1.0/bin/../libs/snappy-java-1.1.8.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/trogdor-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/zookeeper-3.6.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/zookeeper-jute-3.6.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/zstd-jni-1.5.0-4.jar org.apache.zookeeper.server.quorum.QuorumPeerMain ./kafka_2.13-3.1.0/config/zookeeper.properties\n",
            "root       21823       1 57 08:20 ?        00:00:05 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.13-3.1.0/bin/../logs/kafkaServer-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-3.1.0/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-3.1.0/bin/../config/log4j.properties -cp /content/kafka_2.13-3.1.0/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/audience-annotations-0.5.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-basic-auth-extension-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-file-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-json-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-mirror-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-mirror-client-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-runtime-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/connect-transforms-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-annotations-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-core-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-databind-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-dataformat-csv-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-datatype-jdk8-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-jaxrs-base-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-jaxrs-json-provider-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-module-jaxb-annotations-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jackson-module-scala_2.13-2.12.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/content/kafka_2.13-3.1.0/bin/../libs/javassist-3.27.0-GA.jar:/content/kafka_2.13-3.1.0/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jaxb-api-2.3.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-client-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-common-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-container-servlet-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-container-servlet-core-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-hk2-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jersey-server-2.34.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-client-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-continuation-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-http-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-io-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-security-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-server-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-servlet-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-servlets-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-util-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jetty-util-ajax-9.4.43.v20210629.jar:/content/kafka_2.13-3.1.0/bin/../libs/jline-3.12.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/jose4j-0.7.8.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka_2.13-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-clients-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-log4j-appender-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-metadata-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-raft-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-server-common-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-shell-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-storage-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-storage-api-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-examples-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-scala_2.13-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-streams-test-utils-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/kafka-tools-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/log4j-1.2.17.jar:/content/kafka_2.13-3.1.0/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/maven-artifact-3.8.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-buffer-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-codec-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-common-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-handler-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-resolver-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-native-epoll-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/netty-transport-native-unix-common-4.1.68.Final.jar:/content/kafka_2.13-3.1.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-3.1.0/bin/../libs/plexus-utils-3.2.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-3.1.0/bin/../libs/rocksdbjni-6.22.1.1.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-collection-compat_2.13-2.4.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-java8-compat_2.13-1.0.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-library-2.13.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-logging_2.13-3.9.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/scala-reflect-2.13.6.jar:/content/kafka_2.13-3.1.0/bin/../libs/slf4j-api-1.7.30.jar:/content/kafka_2.13-3.1.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/content/kafka_2.13-3.1.0/bin/../libs/snappy-java-1.1.8.4.jar:/content/kafka_2.13-3.1.0/bin/../libs/trogdor-3.1.0.jar:/content/kafka_2.13-3.1.0/bin/../libs/zookeeper-3.6.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/zookeeper-jute-3.6.3.jar:/content/kafka_2.13-3.1.0/bin/../libs/zstd-jni-1.5.0-4.jar kafka.Kafka ./kafka_2.13-3.1.0/config/server.properties\n"
          ]
        }
      ],
      "source": [
        "# 1) Ekstrak\n",
        "!tar -xzf kafka_2.13-3.1.0.tgz\n",
        "\n",
        "# 2) Start ZooKeeper (background)\n",
        "!./kafka_2.13-3.1.0/bin/zookeeper-server-start.sh -daemon \\\n",
        "      ./kafka_2.13-3.1.0/config/zookeeper.properties\n",
        "\n",
        "# 3) Start Kafka broker (background)\n",
        "!./kafka_2.13-3.1.0/bin/kafka-server-start.sh -daemon \\\n",
        "      ./kafka_2.13-3.1.0/config/server.properties\n",
        "\n",
        "# 4) Tunggu sebentar agar service siap\n",
        "!echo \"Waiting 10 s for Kafka & ZooKeeper to be up…\"\n",
        "!sleep 10\n",
        "\n",
        "# 5) Cek proses (harus muncul org.apache.zookeeper.server & kafka.Kafka)\n",
        "!ps -ef | grep -E 'zookeeper|kafka.Kafka' | grep -v grep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Once the instances are started as daemon processes, grep for `kafka` in the processes list. The two java processes correspond to zookeeper and the kafka instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "48LqMJ1BEHm5",
        "outputId": "d2a7b02c-0dd1-42fc-ad6c-a5ecd3a6019b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       15589    1135  0 07:58 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root       15591   15589  0 07:58 ?        00:00:00 grep kafka\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh"
      },
      "source": [
        "Create the kafka topics with the following specs:\n",
        "\n",
        "- susy-train: partitions=1, replication-factor=1\n",
        "- susy-test: partitions=2, replication-factor=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lXJWqMmWnPyP",
        "outputId": "1b7f7629-6c92-4172-9a7f-29fc9faf4db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./kafka_2.13-3.1.0/bin/kafka-topics.sh: No such file or directory\n",
            "/bin/bash: line 1: ./kafka_2.13-3.1.0/bin/kafka-topics.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic susy-train\n",
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic susy-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC"
      },
      "source": [
        "Describe the topic for details on the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "apCf9pfVnwn7",
        "outputId": "e144e7bf-a23e-43d5-d926-8af3ee9ffd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./kafka_2.13-3.1.0/bin/kafka-topics.sh: No such file or directory\n",
            "/bin/bash: line 1: ./kafka_2.13-3.1.0/bin/kafka-topics.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic susy-train\n",
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic susy-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVnz3Pjot9t"
      },
      "source": [
        "The replication factor 1 indicates that the data is not being replicated. This is due to the presence of a single broker in our kafka setup.\n",
        "In production systems, the number of bootstrap servers can be in the range of 100's of nodes. That is where the fault-tolerance using replication comes into picture.\n",
        "\n",
        "Please refer to the [docs](https://kafka.apache.org/documentation/#replication) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCy3zaCQJ7-"
      },
      "source": [
        "## SUSY Dataset\n",
        "\n",
        "Kafka being an event streaming platform, enables  data from various sources to be written into it. For instance:\n",
        "\n",
        "- Web traffic logs\n",
        "- Astronomical measurements\n",
        "- IoT sensor data\n",
        "- Product reviews and many more.\n",
        "\n",
        "For the purpose of this tutorial, lets download the [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY#) dataset and feed the data into kafka manually. The goal of this classification problem is to distinguish between a signal process which produces supersymmetric particles and a background process which does not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "emslB2EGQMCR"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7"
      },
      "source": [
        "### Explore the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18aR_MsOKToc"
      },
      "source": [
        "The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features).\n",
        "The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last 10 features are functions of the first 8 features. These are high-level features derived by physicists to help discriminate between the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "XkXyocIdKRSB"
      },
      "outputs": [],
      "source": [
        "COLUMNS = [\n",
        "          #  labels\n",
        "           'class',\n",
        "          #  low-level features\n",
        "           'lepton_1_pT',\n",
        "           'lepton_1_eta',\n",
        "           'lepton_1_phi',\n",
        "           'lepton_2_pT',\n",
        "           'lepton_2_eta',\n",
        "           'lepton_2_phi',\n",
        "           'missing_energy_magnitude',\n",
        "           'missing_energy_phi',\n",
        "          #  high-level derived features\n",
        "           'MET_rel',\n",
        "           'axial_MET',\n",
        "           'M_R',\n",
        "           'M_TR_2',\n",
        "           'R',\n",
        "           'MT2',\n",
        "           'S_R',\n",
        "           'M_Delta_R',\n",
        "           'dPhi_r_b',\n",
        "           'cos(theta_r1)'\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0NBA51_1Ie2"
      },
      "source": [
        "The entire dataset consists of 5 million rows. However, for the purpose of this tutorial, let's consider only a fraction of the dataset (100,000 rows) so that less time is spent on the moving the data and more time on understanding the functionality of the api."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nC-yt_c9u0sH",
        "outputId": "36e21c52-c310-406d-e75b-b74679628ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class  lepton_1_pT  lepton_1_eta  lepton_1_phi  lepton_2_pT  lepton_2_eta  \\\n",
              "0    0.0     0.972861      0.653855      1.176225     1.157156     -1.739873   \n",
              "1    1.0     1.667973      0.064191     -1.225171     0.506102     -0.338939   \n",
              "2    1.0     0.444840     -0.134298     -0.709972     0.451719     -1.613871   \n",
              "3    1.0     0.381256     -0.976145      0.693152     0.448959      0.891753   \n",
              "4    1.0     1.309996     -0.690089     -0.676259     1.589283     -0.693326   \n",
              "\n",
              "   lepton_2_phi  missing_energy_magnitude  missing_energy_phi   MET_rel  \\\n",
              "0     -0.874309                  0.567765           -0.175000  0.810061   \n",
              "1      1.672543                  3.475464           -1.219136  0.012955   \n",
              "2     -0.768661                  1.219918            0.504026  1.831248   \n",
              "3     -0.677328                  2.033060            1.533041  3.046260   \n",
              "4      0.622907                  1.087562           -0.381742  0.589204   \n",
              "\n",
              "   axial_MET       M_R    M_TR_2         R       MT2       S_R  M_Delta_R  \\\n",
              "0  -0.252552  1.921887  0.889637  0.410772  1.145621  1.932632   0.994464   \n",
              "1   3.775174  1.045977  0.568051  0.481928  0.000000  0.448410   0.205356   \n",
              "2  -0.431385  0.526283  0.941514  1.587535  2.024308  0.603498   1.562374   \n",
              "3  -1.005285  0.569386  1.015211  1.582217  1.551914  0.761215   1.715464   \n",
              "4   1.365479  1.179295  0.968218  0.728563  0.000000  1.083158   0.043429   \n",
              "\n",
              "   dPhi_r_b  cos(theta_r1)  \n",
              "0  1.367815       0.040714  \n",
              "1  1.321893       0.377584  \n",
              "2  1.135454       0.180910  \n",
              "3  1.492257       0.090719  \n",
              "4  1.154854       0.094859  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4891865-9ee0-4844-aafb-ee3261995519\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>lepton_1_pT</th>\n",
              "      <th>lepton_1_eta</th>\n",
              "      <th>lepton_1_phi</th>\n",
              "      <th>lepton_2_pT</th>\n",
              "      <th>lepton_2_eta</th>\n",
              "      <th>lepton_2_phi</th>\n",
              "      <th>missing_energy_magnitude</th>\n",
              "      <th>missing_energy_phi</th>\n",
              "      <th>MET_rel</th>\n",
              "      <th>axial_MET</th>\n",
              "      <th>M_R</th>\n",
              "      <th>M_TR_2</th>\n",
              "      <th>R</th>\n",
              "      <th>MT2</th>\n",
              "      <th>S_R</th>\n",
              "      <th>M_Delta_R</th>\n",
              "      <th>dPhi_r_b</th>\n",
              "      <th>cos(theta_r1)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972861</td>\n",
              "      <td>0.653855</td>\n",
              "      <td>1.176225</td>\n",
              "      <td>1.157156</td>\n",
              "      <td>-1.739873</td>\n",
              "      <td>-0.874309</td>\n",
              "      <td>0.567765</td>\n",
              "      <td>-0.175000</td>\n",
              "      <td>0.810061</td>\n",
              "      <td>-0.252552</td>\n",
              "      <td>1.921887</td>\n",
              "      <td>0.889637</td>\n",
              "      <td>0.410772</td>\n",
              "      <td>1.145621</td>\n",
              "      <td>1.932632</td>\n",
              "      <td>0.994464</td>\n",
              "      <td>1.367815</td>\n",
              "      <td>0.040714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.667973</td>\n",
              "      <td>0.064191</td>\n",
              "      <td>-1.225171</td>\n",
              "      <td>0.506102</td>\n",
              "      <td>-0.338939</td>\n",
              "      <td>1.672543</td>\n",
              "      <td>3.475464</td>\n",
              "      <td>-1.219136</td>\n",
              "      <td>0.012955</td>\n",
              "      <td>3.775174</td>\n",
              "      <td>1.045977</td>\n",
              "      <td>0.568051</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448410</td>\n",
              "      <td>0.205356</td>\n",
              "      <td>1.321893</td>\n",
              "      <td>0.377584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444840</td>\n",
              "      <td>-0.134298</td>\n",
              "      <td>-0.709972</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>-1.613871</td>\n",
              "      <td>-0.768661</td>\n",
              "      <td>1.219918</td>\n",
              "      <td>0.504026</td>\n",
              "      <td>1.831248</td>\n",
              "      <td>-0.431385</td>\n",
              "      <td>0.526283</td>\n",
              "      <td>0.941514</td>\n",
              "      <td>1.587535</td>\n",
              "      <td>2.024308</td>\n",
              "      <td>0.603498</td>\n",
              "      <td>1.562374</td>\n",
              "      <td>1.135454</td>\n",
              "      <td>0.180910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.381256</td>\n",
              "      <td>-0.976145</td>\n",
              "      <td>0.693152</td>\n",
              "      <td>0.448959</td>\n",
              "      <td>0.891753</td>\n",
              "      <td>-0.677328</td>\n",
              "      <td>2.033060</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>3.046260</td>\n",
              "      <td>-1.005285</td>\n",
              "      <td>0.569386</td>\n",
              "      <td>1.015211</td>\n",
              "      <td>1.582217</td>\n",
              "      <td>1.551914</td>\n",
              "      <td>0.761215</td>\n",
              "      <td>1.715464</td>\n",
              "      <td>1.492257</td>\n",
              "      <td>0.090719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.309996</td>\n",
              "      <td>-0.690089</td>\n",
              "      <td>-0.676259</td>\n",
              "      <td>1.589283</td>\n",
              "      <td>-0.693326</td>\n",
              "      <td>0.622907</td>\n",
              "      <td>1.087562</td>\n",
              "      <td>-0.381742</td>\n",
              "      <td>0.589204</td>\n",
              "      <td>1.365479</td>\n",
              "      <td>1.179295</td>\n",
              "      <td>0.968218</td>\n",
              "      <td>0.728563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.083158</td>\n",
              "      <td>0.043429</td>\n",
              "      <td>1.154854</td>\n",
              "      <td>0.094859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4891865-9ee0-4844-aafb-ee3261995519')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4891865-9ee0-4844-aafb-ee3261995519 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4891865-9ee0-4844-aafb-ee3261995519');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0310fbfc-9f5a-4ef4-a2d2-46b3363dca6f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0310fbfc-9f5a-4ef4-a2d2-46b3363dca6f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0310fbfc-9f5a-4ef4-a2d2-46b3363dca6f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "susy_df",
              "summary": "{\n  \"name\": \"susy_df\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4983797962643303,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_1_pT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.682331808823369,\n        \"min\": 0.2556491792201996,\n        \"max\": 12.620587348937988,\n        \"num_unique_values\": 93577,\n        \"samples\": [\n          0.6324176788330078,\n          0.6756335496902466\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_1_eta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0052557999471126,\n        \"min\": -2.102649450302124,\n        \"max\": 2.1014029979705806,\n        \"num_unique_values\": 95686,\n        \"samples\": [\n          -1.7982052564620972,\n          0.13447716832160947\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_1_phi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.003295793173697,\n        \"min\": -1.7347441911697388,\n        \"max\": 1.7347174882888792,\n        \"num_unique_values\": 94585,\n        \"samples\": [\n          -0.3946140110492706,\n          -0.29765552282333374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_2_pT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6496693747701106,\n        \"min\": 0.42859026789665217,\n        \"max\": 13.10712432861328,\n        \"num_unique_values\": 90820,\n        \"samples\": [\n          0.6135015487670897,\n          0.9736295342445372\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_2_eta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0049568971575706,\n        \"min\": -2.0592403411865234,\n        \"max\": 2.059317111968994,\n        \"num_unique_values\": 95557,\n        \"samples\": [\n          0.3509969711303711,\n          0.18107503652572632\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lepton_2_phi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0007899304385943,\n        \"min\": -1.7338486909866333,\n        \"max\": 1.7346696853637695,\n        \"num_unique_values\": 94592,\n        \"samples\": [\n          -0.5122476816177368,\n          0.25016540288925165\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_energy_magnitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8768017016455182,\n        \"min\": 0.0007088767597451806,\n        \"max\": 13.959187507629395,\n        \"num_unique_values\": 94568,\n        \"samples\": [\n          0.5114210844039917,\n          1.2878282070159912\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_energy_phi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0016080474998492,\n        \"min\": -1.727111577987671,\n        \"max\": 1.7406724691390991,\n        \"num_unique_values\": 94683,\n        \"samples\": [\n          1.1342846155166624,\n          1.5985201597213743\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MET_rel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8878219987178354,\n        \"min\": 9.238941629519105e-07,\n        \"max\": 14.50629425048828,\n        \"num_unique_values\": 95674,\n        \"samples\": [\n          5.297459125518799,\n          0.23323965072631836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"axial_MET\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0052870473159972,\n        \"min\": -10.582791328430176,\n        \"max\": 14.342723846435547,\n        \"num_unique_values\": 97150,\n        \"samples\": [\n          -0.9010034203529358,\n          0.08291962742805481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M_R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6241525067632134,\n        \"min\": 0.27296727895736694,\n        \"max\": 13.969072341918944,\n        \"num_unique_values\": 87837,\n        \"samples\": [\n          1.2148388624191284,\n          1.546295404434204\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M_TR_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5849309512847901,\n        \"min\": 0.009538350626826286,\n        \"max\": 13.200295448303223,\n        \"num_unique_values\": 93921,\n        \"samples\": [\n          0.741735577583313,\n          1.5104044675827026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4713501463712043,\n        \"min\": 0.018477957695722576,\n        \"max\": 5.335463523864745,\n        \"num_unique_values\": 94114,\n        \"samples\": [\n          0.380696028470993,\n          1.209713101387024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MT2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8604364442151655,\n        \"min\": 0.0,\n        \"max\": 12.791473388671877,\n        \"num_unique_values\": 75732,\n        \"samples\": [\n          1.6580473184585571,\n          2.8573689460754395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S_R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6162903083973305,\n        \"min\": 0.074356809258461,\n        \"max\": 14.024200439453123,\n        \"num_unique_values\": 89056,\n        \"samples\": [\n          0.9170779585838317,\n          1.0838731527328491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M_Delta_R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6241706674356493,\n        \"min\": 0.004657105542719364,\n        \"max\": 10.329440116882324,\n        \"num_unique_values\": 94875,\n        \"samples\": [\n          1.6417455673217771,\n          0.6305034160614014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dPhi_r_b\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4363005414062368,\n        \"min\": 8.22400670585921e-06,\n        \"max\": 1.591659903526306,\n        \"num_unique_values\": 84516,\n        \"samples\": [\n          1.2333173751831055,\n          1.548124074935913\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos(theta_r1)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19770762381360263,\n        \"min\": 2.3886698841124604e-07,\n        \"max\": 1.0,\n        \"num_unique_values\": 95040,\n        \"samples\": [\n          0.3927310109138489,\n          0.24495799839496607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "susy_iterator = pd.read_csv('SUSY.csv.gz', header=None, names=COLUMNS, chunksize=100000)\n",
        "susy_df = next(susy_iterator)\n",
        "susy_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AlNuW7xbu6o8",
        "outputId": "e0b0e7e7-87b6-4ba0-84fa-903a66631a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Number of datapoints and columns\n",
        "len(susy_df), len(susy_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "c6Cg22bU0-na",
        "outputId": "acb6a2ca-3772-42a2-be4d-c9894117c054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54025, 45975)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Number of datapoints belonging to each class (0: background noise, 1: signal)\n",
        "len(susy_df[susy_df[\"class\"]==0]), len(susy_df[susy_df[\"class\"]==1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P"
      },
      "source": [
        "### Split the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "n-ku_X0Wld59",
        "outputId": "61fee959-fcd8-4a0a-d738-098073747424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  60000\n",
            "Number of testing sample:  40000\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df = train_test_split(susy_df, test_size=0.4, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"class\"], axis=1)\n",
        "y_train_df = train_df[\"class\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"class\"], axis=1)\n",
        "y_test_df = test_df[\"class\"]\n",
        "\n",
        "# The labels are set as the kafka message keys so as to store data\n",
        "# in multiple-partitions. Thus, enabling efficient data retrieval\n",
        "# using the consumer groups.\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YHXk0x2MXVgL",
        "outputId": "67a009ce-5602-4152-fd43-eda8b2532377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 60000, 40000, 40000)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL"
      },
      "source": [
        "### Store the train and test data in kafka\n",
        "\n",
        "Storing the data in kafka simulates an environment for continuous remote data retrieval for training and inference purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YhwFImSqncLE",
        "outputId": "9cd8f056-4a0d-4098-b6f8-11acf7b8b140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.04 secs\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.09 secs\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.20 secs\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.36 secs\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.77 secs\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connect attempt returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-4, node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.54 secs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NoBrokersAvailable",
          "evalue": "NoBrokersAvailable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b60e43562304>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrote {0} messages into topic: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwrite_to_kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"susy-train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mwrite_to_kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"susy-test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-b60e43562304>\u001b[0m in \u001b[0;36mwrite_to_kafka\u001b[0;34m(topic_name, items)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_to_kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mproducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'127.0.0.1:9092'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_errback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         client = self.config['kafka_client'](\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_group_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'producer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Check Broker Version if not set explicitly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBROKER_API_VERSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_versions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBROKER_API_VERSIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[0;34m(self, node_id, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNodeNotReadyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
          ]
        }
      ],
      "source": [
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"susy-test\", zip(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58q52py93jEf"
      },
      "source": [
        "### Define the tfio train dataset\n",
        "\n",
        "The `IODataset` class is utilized for streaming data from kafka into tensorflow. The class inherits from `tf.data.Dataset` and thus has all the useful functionalities of `tf.data.Dataset` out of the box.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHOcitbW2_d1"
      },
      "outputs": [],
      "source": [
        "def decode_kafka_item(item):\n",
        "  message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(item.key)\n",
        "  return (message, key)\n",
        "\n",
        "BATCH_SIZE=64\n",
        "SHUFFLE_BUFFER_SIZE=64\n",
        "train_ds = tfio.IODataset.from_kafka('susy-train', partition=0, offset=0)\n",
        "train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "train_ds = train_ds.map(decode_kafka_item)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x84lZJY164RI"
      },
      "source": [
        "## Build and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHtpAMqLqmv"
      },
      "outputs": [],
      "source": [
        "# Set the parameters\n",
        "\n",
        "OPTIMIZER=\"adam\"\n",
        "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "METRICS=['accuracy']\n",
        "EPOCHS=10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lBmxxuj63jZ"
      },
      "outputs": [],
      "source": [
        "# design/build the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(NUM_COLUMNS,)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTDFVxpSLfXI"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIJMg-saLgeR"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "model.fit(train_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPy0Ka21QII5"
      },
      "source": [
        "Note: Please do not confuse the training step with online training. It's an entirely different paradigm which will be covered in a later section.\n",
        "\n",
        "Since only a fraction of the dataset is being utilized, our accuracy is limited to ~78% during the training phase. However, please feel free to store additional data in kafka for a better model performance. Also, since the goal was to just demonstrate the functionality of the tfio kafka datasets, a smaller and less-complicated neural network was used. However, one can increase the complexity of the model, modify the learning strategy, tune hyper-parameters etc for exploration purposes. For a baseline approach, please refer to this [article](https://www.nature.com/articles/ncomms5308#Sec11)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJW8za2qm4c"
      },
      "source": [
        "## Infer on the test data\n",
        "\n",
        "To infer on the test data by adhering to the 'exactly-once' semantics along with fault-tolerance, the `streaming.KafkaGroupIODataset` can be utilized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3FZOlSh2pmy"
      },
      "source": [
        "### Define the tfio test dataset\n",
        "\n",
        "The `stream_timeout` parameter blocks for the given duration for new data points to be streamed into the topic. This removes the need for creating new datasets if the data is being streamed into the topic in an intermittent fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjnM81lPROen"
      },
      "outputs": [],
      "source": [
        "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-test\"],\n",
        "    group_id=\"testcg\",\n",
        "    servers=\"127.0.0.1:9092\",\n",
        "    stream_timeout=10000,\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_test_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "test_ds = test_ds.map(decode_kafka_test_item)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8j3bZsSF6u"
      },
      "source": [
        "Though this class can be used for training purposes, there are caveats which need to be addressed. Once all the messages are read from kafka and the latest offsets are committed using the `streaming.KafkaGroupIODataset`, the consumer doesn't restart reading the messages from the beginning. Thus, while training, it is possible only to train for a single epoch with the data continuously flowing in. This kind of a functionality has limited use cases during the training phase wherein, once a datapoint has been consumed by the model it is no longer required and can be discarded.\n",
        "\n",
        "However, this functionality shines when it comes to robust inference with exactly-once semantics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PapN5Q_241k"
      },
      "source": [
        "### evaluate the performance on the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hMtIe1X215P"
      },
      "outputs": [],
      "source": [
        "res = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWX9j11bWJGe"
      },
      "source": [
        "Since the inference is based on 'exactly-once' semantics, the evaluation on the test set can be run only once. In order to run the inference again on the test data, a new consumer group should be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Chcbd9xThl"
      },
      "source": [
        "### Track the offset lag of the `testcg` consumer group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uz3km0RxUG7"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --group testcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Wg0_eXMKL9"
      },
      "source": [
        "Once the `current-offset` matches the `log-end-offset` for all the partitions, it indicates that the consumer(s) have completed fetching all the messages from the kafka topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYwillcxP97z"
      },
      "source": [
        "## Online learning\n",
        "\n",
        "The online machine learning paradigm is a bit different from the traditional/conventional way of training machine learning models. In the former case, the model continues to incrementally learn/update it's parameters as soon as the new data points are available and this process is expected to continue indefinitely. This is unlike the latter approaches where the dataset is fixed and the model iterates over it `n` number of times. In online learning, the data once consumed by the model may not be available for training again.\n",
        "\n",
        "By utilizing the `streaming.KafkaBatchIODataset`, it is now possible to train the models in this fashion. Let's continue to use our SUSY dataset for demonstrating this functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5HyQtUZXi_P"
      },
      "source": [
        "### The tfio training dataset for online learning\n",
        "\n",
        "The `streaming.KafkaBatchIODataset` is similar to the `streaming.KafkaGroupIODataset` in it's API. Additionally, it is recommended to utilize the `stream_timeout` parameter to configure the duration for which the dataset will block for new messages before timing out. In the instance below, the dataset is configured with a `stream_timeout` of `10000` milliseconds. This implies that, after all the messages from the topic have been consumed, the dataset will wait for an additional 10 seconds before timing out and disconnecting from the kafka cluster. If new messages are streamed into the topic before timing out, the data consumption and model training resumes for those newly consumed data points. To block indefinitely, set it to `-1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-zCHNOuSJDL"
      },
      "outputs": [],
      "source": [
        "online_train_ds = tfio.experimental.streaming.KafkaBatchIODataset(\n",
        "    topics=[\"susy-train\"],\n",
        "    group_id=\"cgonline\",\n",
        "    servers=\"127.0.0.1:9092\",\n",
        "    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgSCn5dskO0t"
      },
      "source": [
        "Every item that the `online_train_ds` generates is a `tf.data.Dataset` in itself. Thus, all the standard transformations can be applied as usual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cxF0bgGkQJs"
      },
      "outputs": [],
      "source": [
        "def decode_kafka_online_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "for mini_ds in online_train_ds:\n",
        "  mini_ds = mini_ds.shuffle(buffer_size=32)\n",
        "  mini_ds = mini_ds.map(decode_kafka_online_item)\n",
        "  mini_ds = mini_ds.batch(32)\n",
        "  if len(mini_ds) > 0:\n",
        "    model.fit(mini_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGph8eP9isuW"
      },
      "source": [
        "The incrementally trained model can be saved in a periodic fashion (based on use-cases) and can be utilized to infer on the test data in either online or offline modes.\n",
        "\n",
        "Note: The `streaming.KafkaBatchIODataset` and `streaming.KafkaGroupIODataset` are still in experimental phase and have scope for improvements based on user-feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QAS_3k1y3u"
      },
      "source": [
        "## References:\n",
        "\n",
        "- Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014)\n",
        "\n",
        "- SUSY Dataset: https://archive.ics.uci.edu/ml/datasets/SUSY#\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kafka.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}